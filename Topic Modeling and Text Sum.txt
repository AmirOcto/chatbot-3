Topic modeling is a statistical method for discovering the hidden thematic structure (main themes) in a collection of documents. Topic modeling algorithms are unsupervised learning: they do not require any prior annotations or labeling of the documents, the topics emerge from the analysis of the original texts, and documents and words are clustered simultaneously. Topic models can then organize the collection according to the discovered themes. Topic models can help identifying variables or features. Topic is the subject (theme) of a discourse where each topic is represented as a distribution over words. A document is assumed to be a mixture of topics. Basic assumptions on which all topic modeling algorithms are based: each document consists of more than one topic, and each topic consists of a collection of words. Topic modeling algorithms are built around the idea that the semantics of our document is actually being governed by some hidden or latent variables that we are not observing directly after seeing the textual material. The output of the topic model is a list of words associated with each topic with high probability and an assignment of each document to topics. Topic Modeling techniques include Latent Semantic Analysis (LSA) or LSI (indexing), Latent Dirichlet Allocation (LDA), and others. LSA takes a matrix of documents and terms (m x n) and tries to decompose it into separate two matrices: a document topic matrix and a topic term matrix. The learning of LSA for latent topics includes matrix decomposition on the document term matrix using Singular value decomposition (SVD). The idea of SVD is finding the most valuable information and using lower dimension t to represent the same thing (it is a dimension reduction or noise reducing technique). The limitation of LSA is the computational cost of the SVD. Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation[1][2] is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically. In natural language processing, Latent Dirichlet Allocation (LDA) is a generative statistical model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar. The LDA is an example of a topic model. In this, observations (e.g., words) are collected into documents, and each word's presence is attributable to one of the document's topics. Each document will contain a small number of topics. LDA will go through each of the words in each of the documents, and it will randomly assign the word to one of the K topics. Since assignment is done randomly, LDA will compute p(topic t | document d) the percentage of words within the document that were assigned to a particular topic and p(word w | topic t) percentage of times the word w was assigned to topic t over all documents. LDA will decide to move a word w from topic A to topic B when p(topic A | document d) * p(word w | topic A) < p(topic B | document d) *p(word w |topic B). After a while, LDA "converges" to a more optimal state, where topic representations and documents represented in terms of these topics are ok. The parameters that should be fine tuned in LDA are the number of topics, document topic density alpha, and topic word density beta. High values of alpha in LDA means that documents are composed of more topics. High values of beta in LDA means that topics are composed of a large number of words in the corpus. The evaluation of LDA Evaluation includes perplexity, coherence, topics interpretability, topics uniqueness, and topics exhaustivity. The limitations of LDA include lots of fine tuning, requiring human interpretation, and the inability to influence topics. Automatic text summarization aims to transform lengthy documents into shortened, accurate, and coherent versions. The goal is to have summaries as good as those written by humans. Text summarization is the practice of breaking down long publications into manageable paragraphs or sentences. The procedure extracts important information while also ensuring that the paragraph's sense is preserved. This shortens the time it takes to comprehend long materials like research articles while without omitting critical information. The process of constructing a concise, cohesive, and fluent summary of a lengthier text document, which includes highlighting the text's important points, is known as text summarization. Text summarizing presents a number of issues, including text identification, interpretation, and summary generation, as well as analysis of the resulting summary. Identifying important phrases in the document and exploiting them to uncover relevant information to add in the summary are critical jobs in extraction-based summarizing. There are 2 approaches for text summarization: extractive text summarizing approach and abstractive summarization. The extractive text summarizing approach entails extracting essential words from a source material and combining them to create a summary. Abstractive Text Summarization is the task of generating a short and concise summary that captures the salient ideas of the source text, where the generated summaries potentially contain new phrases and sentences that may not appear in the source text. Single document summarization: given a single document, produce abstract, outline, and headline whereas multiple document summarization: given a group of documents, produce a gist of the content: a series of news stories on the same event or a set of web pages about some topic or question. Generic summarization summarizes the content of a document whereas query focused summarization summarizes a document with respect to information expressed in a user query. Summarization constitutes of three stages: content selection (choose sentences to extract from the document), information ordering (choose an order to place them in the summary), and sentence realization (clean up the sentences). PageRank (PR) is an algorithm used by Google Search to rank web pages in their search engine results. It is named after both the term "web page" and co-founder Larry Page. PageRank is a way of measuring the importance of website pages. According to Google: PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. Currently, PageRank is not the only algorithm used by Google to order search results, but it is the first algorithm that was used by the company, and it is the best known.[2][3] As of September 24, 2019, PageRank and all associated patents are expired. Textrank is a graph-based ranking algorithm like Google's PageRank algorithm which has been successfully implemented in citation analysis. We use text rank often for keyword extraction, automated text summarization and phrase ranking. TextRank algorithm functions as follows: parse text into sentences, make a graph node for every sentence in the text, draw an edge between every pair of nodes, calculate weights for each edge using a similarity measure, calculate for each node/sentence an importance weight via the PageRank algorithm, and concatenate sentences in decreasing order of importance until some word limit is reached. Text summarization is evaluated by ROUGE (Recall Oriented Understudy for Gisting Evaluation). ROUGE is a set of metrics for evaluating automatic summarization of texts as well as machine translations and it works by comparing an automatically produced summary or translation against a set of reference summaries (typically human produced). ROUGE-n is a measure of n gram overlap between a summary and a set of reference summaries. ROUGE-L uses longest common subsequence instead of n gram overlap where the longest sequence of words is not necessarily consecutive, but still in order. 