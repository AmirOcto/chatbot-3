Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers). An information need is the topic about which the user desires to know more about, a query is what the user conveys to the computer in an attempt to communicate the information need, and a document is relevant if the user perceives that it contains information of value with respect to their personal information need. The steps of information retrieval include indexation, query preprocessing, and then performing text similarity for matching. The indexation process is formed of the following steps: structure analysis and tokenization, stopwords removal, morphological normalization, and weighting. The query processing will undergo the same steps followed in indexing. Informally, similarity between two objects (e g two images, two documents, two records, etc is a numerical measure of the degree to which two objects are alike. The dissimilarity on the other hand, is another alternative (or opposite) measure of the degree to which two objects are different. Both similarity and dissimilarity are termed as proximity. The challenges of information retrieval constitute of using stemmers, spelling errors, syntax, and semantics. Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science[1] of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds. An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevance. An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching. Depending on the application the data objects may be, for example, text documents, images,[3] audio,[4] mind maps[5] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata. The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945.[7] It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film.[8] The first description of a computer searching for information was described by Holmstrom in 1948,[9] detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents).[7] Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.

In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further. Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications. The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval[clarification needed] or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevance. Clustering is the process of grouping a set of objects into classes of similar objects. Document clustering is grouping documents (records) into groups of related/similar documents where documents within a cluster should be similar and documents from different clusters should be dissimilar. Clustering algorithms in computational text analysis groups documents into grouping a set of text what are called subsets or clusters where the algorithm's goal is to create internally coherent clusters that are distinct from one another whereas classification on the other hand, is a form of supervised learning where the features of the documents are used to predict the "type" or class of documents. Text clustering is evaluated by 2 criteria: internal criterion where a good cluster is the one having a high intracluster similarity, a low interclass similarity, and the measured quality depends on both the document representation and the similarity measure; in addition to an external criterion where the quality of a clustering is also measured by its ability to discover some or all of the hidden patterns or latent classes (common external metrics: Purity, Entropy). Cluster cohesion is the sum of the weight of all links within a cluster. Cluster separation is the sum of the weights between nodes in the cluster and nodes outside the cluster. Silhouette Coefficient or silhouette score is a metric used to quantify how good is a clustering technique (its value ranges from -1 to 1) where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. Purity is the ratio between the dominant class in the cluster and the size of the cluster where a bad clustering has purity values close to 0 and a perfect clustering has a purity of 1. Entropy is a measure of the randomness in the information being processed where a perfect clustering leads to an entropy of 0 and the worst clustering gives a value of log k (k is the number of clusters). There are 2 types of clustering algorithms: partitioning “flat” algorithms (k-means/medoids clustering, model-based clustering) and hierarchical algorithms (bottom up-agglomerative, top down-divisive). K-means clustering uses “centroids”, K different randomly-initiated points in the data, and assigns every data point to the nearest centroid, such that after every point being assigned, the centroid is moved to the average of all of the points assigned to it. Then, the process repeats: every point is assigned to its nearest centroid, centroids are moved to the average of points assigned to it until convergence or reaching maximum number of iterations. Affinity propagation is an algorithm that builds clusters based on inherent properties of the data without any assumptions about the number of clusters. Affinity propagation is iterative which means that it will complete a number of iterations until completion. For a set of data points a "group formation" process begins, where each sample competes with other ones in order to gain group membership. The ones with most group capital, the group leaders are called exemplars. Affinity propagation is based on the concept of "message passing" among the various data points to be clustered. During each iteration, each sample broadcasts two types of messages to the other samples: responsibility (evidence that sample k should be the exemplar for sample and availability (how certain i would choose k as the exemplar, i.e., how available it is to join a particular group). The entire dataset is then represented by a small number of exemplars where these exemplars are analogous to the centroids (K means). The drawback of affinity prorogation is that it is computationally intensive. Agglomerative (bottom up) clustering starts with each document being a single cluster and eventually all documents will belong to the same cluster. Divisive (top down) starts with all documents belonging to the same cluster and eventually each node will form a cluster on its own. Hierarchical clustering does not require the number of clusters k in advance, but requires a threshold. 
